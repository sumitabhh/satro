Chapter 5: Sorting Algorithms

Introduction to Sorting:
Sorting is a fundamental operation in computer science that arranges elements in a specific order (typically ascending or descending). Efficient sorting is crucial for optimizing many other algorithms that require sorted data as input.

Bubble Sort:
Bubble sort is one of the simplest sorting algorithms. It repeatedly steps through the list, compares adjacent elements and swaps them if they're in the wrong order.

Algorithm:
1. Start from the first element
2. Compare current element with next element
3. If current > next, swap them
4. Move to next element
5. Repeat until end of list
6. Continue until no swaps are needed

Time Complexity: O(n²)
Space Complexity: O(1)
Best for: Small datasets, educational purposes

QuickSort:
QuickSort is an efficient divide-and-conquer algorithm developed by Tony Hoare in 1959. It's generally faster than other O(n log n) algorithms like heap sort or merge sort.

Algorithm:
1. Choose a pivot element
2. Partition the array: elements < pivot, elements = pivot, elements > pivot
3. Recursively apply QuickSort to sub-arrays
4. Combine results

Time Complexity:
- Best: O(n log n)
- Average: O(n log n)
- Worst: O(n²)

Space Complexity: O(log n) average case

Key Concepts:
- Pivot selection affects performance
- Good for cache performance
- Widely used in practice

Merge Sort:
Merge Sort is another divide-and-conquer algorithm that divides the array into halves, recursively sorts them, and then merges the sorted halves.

Algorithm:
1. Divide array into two halves
2. Recursively sort each half
3. Merge the sorted halves

Time Complexity: O(n log n) in all cases
Space Complexity: O(n)
Stable: Yes (maintains relative order of equal elements)

Applications:
- External sorting (large datasets)
- Stable sorting requirements
- Parallel processing友好

Summary:
- Bubble Sort: Simple but inefficient
- QuickSort: Fast on average, worst case O(n²)
- Merge Sort: Consistent O(n log n) performance, requires extra space